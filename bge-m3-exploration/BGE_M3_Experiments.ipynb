{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Kicking the Tires on BGE-M3\n",
    "\n",
    "Experiments from the blog post. Testing all three modes against 100K property listings.\n",
    "\n",
    "**Why Python instead of SQL Server?** SQL Server 2025's VECTOR_DISTANCE only works with dense embeddings. To test sparse and ColBERT, I pulled data into Python and used FlagEmbedding directly.\n",
    "\n",
    "**Data source:** [SemanticSonarDB](https://github.com/MrJoeSack/sqlserver-sample-databases/tree/master/semantic-sonar-db) - 100K synthetic property listings for SQL Server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-install",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (run once)\n",
    "# pip install FlagEmbedding pyodbc numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import time\n",
    "import pyodbc\n",
    "import numpy as np\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "\n",
    "def get_conn():\n",
    "    \"\"\"Connect to SemanticSonarDB. Update connection string for your environment.\"\"\"\n",
    "    return pyodbc.connect(\n",
    "        \"DRIVER={ODBC Driver 18 for SQL Server};\"\n",
    "        \"SERVER=localhost;DATABASE=SemanticSonarDB;\"\n",
    "        \"TrustServerCertificate=yes;Trusted_Connection=yes;\"\n",
    "    )\n",
    "\n",
    "print(\"Loading BGE-M3...\")\n",
    "start = time.time()\n",
    "model = BGEM3FlagModel('BAAI/bge-m3', use_fp16=False)\n",
    "print(f\"Loaded in {time.time() - start:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "conn = get_conn()\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# 50 random listings for tests\n",
    "cursor.execute(\"\"\"\n",
    "    SELECT TOP 50 property_id, listing_description\n",
    "    FROM PropertyListings\n",
    "    ORDER BY NEWID()\n",
    "\"\"\")\n",
    "docs = [(r[0], r[1]) for r in cursor.fetchall()]\n",
    "conn.close()\n",
    "\n",
    "print(f\"Loaded {len(docs)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scoring functions for each mode\n",
    "\n",
    "def dense_score(q_emb, d_emb):\n",
    "    \"\"\"Cosine similarity between query and document dense vectors.\"\"\"\n",
    "    return float(q_emb @ d_emb.T)\n",
    "\n",
    "def sparse_score(q_sparse, d_sparse):\n",
    "    \"\"\"Sum of minimum weights for tokens appearing in both query and document.\"\"\"\n",
    "    score = 0.0\n",
    "    for token in q_sparse:\n",
    "        if token in d_sparse:\n",
    "            score += min(q_sparse[token], d_sparse[token])\n",
    "    return score\n",
    "\n",
    "def colbert_score(q_vecs, d_vecs):\n",
    "    \"\"\"MaxSim: for each query token, find max similarity to any document token, then sum.\"\"\"\n",
    "    return model.colbert_score(q_vecs, d_vecs)\n",
    "\n",
    "def rank_all_modes(query, docs, doc_output):\n",
    "    \"\"\"Rank documents by each mode separately. Returns dict with sorted results per mode.\"\"\"\n",
    "    q_out = model.encode([query], return_dense=True, return_sparse=True, return_colbert_vecs=True)\n",
    "    \n",
    "    results = {'dense': [], 'sparse': [], 'colbert': []}\n",
    "    for i, (pid, text) in enumerate(docs):\n",
    "        results['dense'].append((pid, dense_score(q_out['dense_vecs'][0], doc_output['dense_vecs'][i])))\n",
    "        results['sparse'].append((pid, sparse_score(q_out['lexical_weights'][0], doc_output['lexical_weights'][i])))\n",
    "        results['colbert'].append((pid, colbert_score(q_out['colbert_vecs'][0], doc_output['colbert_vecs'][i])))\n",
    "    \n",
    "    for mode in results:\n",
    "        results[mode] = sorted(results[mode], key=lambda x: x[1], reverse=True)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "---\n",
    "## Three Methods, Same Query\n",
    "\n",
    "Each mode picks a different winner. Didn't expect that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode all docs with all three modes\n",
    "print(\"Encoding documents...\")\n",
    "doc_output = model.encode([d[1] for d in docs], return_dense=True, return_sparse=True, return_colbert_vecs=True)\n",
    "\n",
    "query = \"cozy home with fireplace\"\n",
    "results = rank_all_modes(query, docs, doc_output)\n",
    "\n",
    "print(f'\\n\"{query}\"')\n",
    "print(f\"  Dense:   property {results['dense'][0][0]} (score: {results['dense'][0][1]:.3f})\")\n",
    "print(f\"  Sparse:  property {results['sparse'][0][0]} (score: {results['sparse'][0][1]:.3f})\")\n",
    "print(f\"  ColBERT: property {results['colbert'][0][0]} (score: {results['colbert'][0][1]:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "---\n",
    "## French Query, English Docs\n",
    "\n",
    "This one was interesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    (\"English\", \"cozy home with fireplace\"),\n",
    "    (\"French\", \"maison confortable avec cheminee\"),\n",
    "]\n",
    "\n",
    "for lang, query in queries:\n",
    "    q_out = model.encode([query], return_dense=True, return_sparse=True, return_colbert_vecs=True)\n",
    "    \n",
    "    # Get max scores across all docs\n",
    "    dense_max = max(dense_score(q_out['dense_vecs'][0], doc_output['dense_vecs'][i]) for i in range(len(docs)))\n",
    "    sparse_max = max(sparse_score(q_out['lexical_weights'][0], doc_output['lexical_weights'][i]) for i in range(len(docs)))\n",
    "    \n",
    "    print(f'{lang}: \"{query}\"')\n",
    "    print(f\"  Dense:  {dense_max:.3f}\")\n",
    "    print(f\"  Sparse: {sparse_max:.3f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "Sparse found nothing. No French words in English listings means no lexical overlap. Dense understood \"cheminee\" maps to fireplace semantically.\n",
    "\n",
    "---\n",
    "## Other Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How much slower is ColBERT scoring?\n",
    "q_out = model.encode([\"test query\"], return_dense=True, return_sparse=True, return_colbert_vecs=True)\n",
    "\n",
    "start = time.time()\n",
    "for _ in range(100):\n",
    "    for i in range(len(docs)):\n",
    "        _ = dense_score(q_out['dense_vecs'][0], doc_output['dense_vecs'][i])\n",
    "dense_time = time.time() - start\n",
    "\n",
    "start = time.time()\n",
    "for _ in range(100):\n",
    "    for i in range(len(docs)):\n",
    "        _ = colbert_score(q_out['colbert_vecs'][0], doc_output['colbert_vecs'][i])\n",
    "colbert_time = time.time() - start\n",
    "\n",
    "print(f\"Scoring {len(docs)} docs (100 iterations):\")\n",
    "print(f\"  Dense:   {dense_time:.2f}s\")\n",
    "print(f\"  ColBERT: {colbert_time:.2f}s\")\n",
    "print(f\"  ColBERT is ~{colbert_time/dense_time:.0f}x slower\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
