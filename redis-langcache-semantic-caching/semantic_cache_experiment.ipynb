{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Testing Semantic Cache Embeddings\n\nSemantic caching avoids redundant LLM calls by recognizing when two queries mean the same thing. The challenge is distinguishing \"same question, different words\" from \"related topic, different question.\"\n\n**What we're testing:** Redis released LangCache embedding models fine-tuned specifically for semantic caching. Can they outperform general-purpose embeddings at detecting equivalent queries?\n\n**The scenario:** User A asks \"luxury home with pool.\" User B asks \"upscale property with swimming pool.\" Should these hit the same cache? What about \"luxury home with pool\" vs \"affordable starter home\"?"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## What is Semantic Caching?\n\nTraditional caching uses exact key matching. Semantic caching uses embedding similarity to detect equivalent queries with different words.\n\nThe tradeoff: maximize cache hits for equivalent queries while avoiding false positives (returning wrong cached responses for queries that look similar but have different intent)."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (uncomment if needed)\n",
    "# !pip install sentence-transformers numpy pandas matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    \"\"\"Compute cosine similarity between two vectors.\"\"\"\n",
    "    return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data: Real Estate Query Pairs\n",
    "\n",
    "We need pairs of queries where we know if they should match (cache hit) or not.\n",
    "\n",
    "**Positive pairs:** Different phrasings of the same search intent  \n",
    "**Negative pairs:** Queries that look similar but have different intent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positive pairs - these SHOULD match (same intent, different words)\n",
    "positive_pairs = [\n",
    "    (\"waterfront property with private dock\", \"lakefront home with boat dock\"),\n",
    "    (\"cozy cottage near the beach\", \"small beach house with charm\"),\n",
    "    (\"modern downtown condo with city views\", \"contemporary urban apartment overlooking the city\"),\n",
    "    (\"family home with large backyard\", \"house with big yard for kids\"),\n",
    "    (\"luxury home with pool\", \"upscale property with swimming pool\"),\n",
    "    (\"quiet neighborhood for retirees\", \"peaceful area good for retirement\"),\n",
    "    (\"home with home office space\", \"house with dedicated workspace\"),\n",
    "    (\"pet-friendly apartment\", \"condo that allows dogs\"),\n",
    "    (\"fixer-upper with potential\", \"needs work but good bones\"),\n",
    "    (\"move-in ready home\", \"turnkey property no repairs needed\"),\n",
    "    (\"open floor plan with natural light\", \"bright and airy layout\"),\n",
    "    (\"close to good schools\", \"in a top school district\"),\n",
    "    (\"investment property with rental income\", \"income-generating real estate\"),\n",
    "    (\"historic home with character\", \"older house with original features\"),\n",
    "    (\"energy efficient home with solar\", \"eco-friendly house with solar panels\"),\n",
    "]\n",
    "\n",
    "# Negative pairs - these should NOT match (different intent)\n",
    "negative_pairs = [\n",
    "    (\"waterfront property with private dock\", \"downtown condo near restaurants\"),\n",
    "    (\"cozy cottage near the beach\", \"mountain cabin with ski access\"),\n",
    "    (\"luxury home with pool\", \"affordable starter home\"),\n",
    "    (\"quiet neighborhood for retirees\", \"vibrant area with nightlife\"),\n",
    "    (\"family home with large backyard\", \"studio apartment downtown\"),\n",
    "    (\"fixer-upper with potential\", \"brand new construction\"),\n",
    "    (\"pet-friendly apartment\", \"no pets allowed policy\"),\n",
    "    (\"close to good schools\", \"adult community 55+\"),\n",
    "    (\"historic home with character\", \"modern minimalist new build\"),\n",
    "    (\"investment property with rental income\", \"forever home to raise family\"),\n",
    "]\n",
    "\n",
    "print(f\"Positive pairs (should match): {len(positive_pairs)}\")\n",
    "print(f\"Negative pairs (should NOT match): {len(negative_pairs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Load the Models\n\nWe're comparing:\n1. **redis/langcache-embed-v3-small** - Fine-tuned specifically for semantic caching\n2. **all-MiniLM-L6-v2** - General-purpose sentence embeddings\n\nHere's what makes this interesting: LangCache is actually fine-tuned *from* MiniLM-L6. Same base model, same size (22.6M params, 384 dimensions), different training objective. The LangCache model was trained on sentence-pair data (anchor, positive, negative examples) to better separate \"same intent\" from \"related but different\" queries."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading Redis LangCache model...\")\n",
    "langcache_model = SentenceTransformer(\"redis/langcache-embed-v3-small\")\n",
    "print(f\"  Embedding dimension: {langcache_model.get_sentence_embedding_dimension()}\")\n",
    "\n",
    "print(\"\\nLoading baseline MiniLM model...\")\n",
    "baseline_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "print(f\"  Embedding dimension: {baseline_model.get_sentence_embedding_dimension()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Similarities\n",
    "\n",
    "For each model, we'll compute the cosine similarity between all query pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarities(model, positive_pairs, negative_pairs):\n",
    "    \"\"\"Compute similarities for all pairs using a given model.\"\"\"\n",
    "    # Collect all unique texts\n",
    "    all_texts = []\n",
    "    for p1, p2 in positive_pairs + negative_pairs:\n",
    "        all_texts.extend([p1, p2])\n",
    "    all_texts = list(set(all_texts))\n",
    "    \n",
    "    # Generate embeddings\n",
    "    start = time.time()\n",
    "    embeddings = model.encode(all_texts, show_progress_bar=False)\n",
    "    embed_time = time.time() - start\n",
    "    \n",
    "    # Create lookup\n",
    "    text_to_emb = {t: e for t, e in zip(all_texts, embeddings)}\n",
    "    \n",
    "    # Compute similarities\n",
    "    pos_sims = [cosine_similarity(text_to_emb[p1], text_to_emb[p2]) for p1, p2 in positive_pairs]\n",
    "    neg_sims = [cosine_similarity(text_to_emb[p1], text_to_emb[p2]) for p1, p2 in negative_pairs]\n",
    "    \n",
    "    return pos_sims, neg_sims, embed_time\n",
    "\n",
    "# Run for both models\n",
    "print(\"Computing similarities with LangCache model...\")\n",
    "lc_pos, lc_neg, lc_time = compute_similarities(langcache_model, positive_pairs, negative_pairs)\n",
    "\n",
    "print(\"Computing similarities with baseline model...\")\n",
    "bl_pos, bl_neg, bl_time = compute_similarities(baseline_model, positive_pairs, negative_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results: Similarity Distributions\n",
    "\n",
    "A good caching model should:\n",
    "- Give HIGH similarity scores to positive pairs (cache hits)\n",
    "- Give LOW similarity scores to negative pairs (avoid false hits)\n",
    "- Have clear SEPARATION between the two distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"SIMILARITY DISTRIBUTION COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nRedis LangCache (cache-optimized):\")\n",
    "print(f\"  Positive pairs - Mean: {np.mean(lc_pos):.4f}, Min: {np.min(lc_pos):.4f}, Max: {np.max(lc_pos):.4f}\")\n",
    "print(f\"  Negative pairs - Mean: {np.mean(lc_neg):.4f}, Min: {np.min(lc_neg):.4f}, Max: {np.max(lc_neg):.4f}\")\n",
    "print(f\"  Separation gap: {np.mean(lc_pos) - np.mean(lc_neg):.4f}\")\n",
    "\n",
    "print(\"\\nBaseline MiniLM (general-purpose):\")\n",
    "print(f\"  Positive pairs - Mean: {np.mean(bl_pos):.4f}, Min: {np.min(bl_pos):.4f}, Max: {np.max(bl_pos):.4f}\")\n",
    "print(f\"  Negative pairs - Mean: {np.mean(bl_neg):.4f}, Min: {np.min(bl_neg):.4f}, Max: {np.max(bl_neg):.4f}\")\n",
    "print(f\"  Separation gap: {np.mean(bl_pos) - np.mean(bl_neg):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the distributions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# LangCache\n",
    "axes[0].hist(lc_pos, bins=15, alpha=0.7, label='Positive (should match)', color='green')\n",
    "axes[0].hist(lc_neg, bins=15, alpha=0.7, label='Negative (should NOT match)', color='red')\n",
    "axes[0].axvline(x=0.85, color='black', linestyle='--', label='Threshold 0.85')\n",
    "axes[0].set_xlabel('Cosine Similarity')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('Redis LangCache (cache-optimized)')\n",
    "axes[0].legend()\n",
    "axes[0].set_xlim(0, 1)\n",
    "\n",
    "# Baseline\n",
    "axes[1].hist(bl_pos, bins=15, alpha=0.7, label='Positive (should match)', color='green')\n",
    "axes[1].hist(bl_neg, bins=15, alpha=0.7, label='Negative (should NOT match)', color='red')\n",
    "axes[1].axvline(x=0.85, color='black', linestyle='--', label='Threshold 0.85')\n",
    "axes[1].set_xlabel('Cosine Similarity')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_title('Baseline MiniLM (general-purpose)')\n",
    "axes[1].legend()\n",
    "axes[1].set_xlim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('similarity_distributions.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision/Recall at Different Thresholds\n",
    "\n",
    "The similarity threshold determines when we consider two queries equivalent:\n",
    "- **Too low (0.7):** More cache hits, but also more false positives\n",
    "- **Too high (0.95):** Fewer false positives, but miss valid cache hits\n",
    "\n",
    "Let's find the sweet spot for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_threshold(pos_sims, neg_sims, threshold):\n",
    "    \"\"\"Evaluate precision/recall at a given threshold.\"\"\"\n",
    "    tp = sum(1 for s in pos_sims if s >= threshold)  # Correct cache hits\n",
    "    fn = sum(1 for s in pos_sims if s < threshold)   # Missed cache hits\n",
    "    fp = sum(1 for s in neg_sims if s >= threshold)  # False cache hits\n",
    "    tn = sum(1 for s in neg_sims if s < threshold)   # Correct rejections\n",
    "    \n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return precision, recall, f1\n",
    "\n",
    "thresholds = [0.70, 0.75, 0.80, 0.85, 0.90, 0.95]\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"THRESHOLD COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Threshold':<12} {'LangCache':<28} {'Baseline':<28}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "lc_results = []\n",
    "bl_results = []\n",
    "\n",
    "for t in thresholds:\n",
    "    lc_p, lc_r, lc_f1 = evaluate_threshold(lc_pos, lc_neg, t)\n",
    "    bl_p, bl_r, bl_f1 = evaluate_threshold(bl_pos, bl_neg, t)\n",
    "    \n",
    "    lc_results.append((lc_p, lc_r, lc_f1))\n",
    "    bl_results.append((bl_p, bl_r, bl_f1))\n",
    "    \n",
    "    print(f\"{t:<12.2f} P:{lc_p:.2f} R:{lc_r:.2f} F1:{lc_f1:.2f}    P:{bl_p:.2f} R:{bl_r:.2f} F1:{bl_f1:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot F1 scores across thresholds\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.plot(thresholds, [r[2] for r in lc_results], 'o-', label='Redis LangCache', linewidth=2, markersize=8)\n",
    "plt.plot(thresholds, [r[2] for r in bl_results], 's-', label='Baseline MiniLM', linewidth=2, markersize=8)\n",
    "\n",
    "plt.xlabel('Similarity Threshold', fontsize=12)\n",
    "plt.ylabel('F1 Score', fontsize=12)\n",
    "plt.title('Cache Hit Detection: F1 Score vs Threshold', fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.ylim(0, 1.05)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('f1_comparison.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hardest Cases Analysis\n",
    "\n",
    "Which pairs are causing problems? Let's look at:\n",
    "- **Lowest positive similarities:** Cache misses we shouldn't have\n",
    "- **Highest negative similarities:** False cache hits we need to avoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine pairs with their similarities\n",
    "lc_pos_with_sims = list(zip(positive_pairs, lc_pos))\n",
    "lc_neg_with_sims = list(zip(negative_pairs, lc_neg))\n",
    "\n",
    "print(\"LANGCACHE - Hardest Positive Pairs (lowest similarity, risk of cache miss):\")\n",
    "for (p1, p2), sim in sorted(lc_pos_with_sims, key=lambda x: x[1])[:3]:\n",
    "    print(f\"  {sim:.4f}: '{p1}' vs '{p2}'\")\n",
    "\n",
    "print(\"\\nLANGCACHE - Riskiest Negative Pairs (highest similarity, risk of false hit):\")\n",
    "for (p1, p2), sim in sorted(lc_neg_with_sims, key=lambda x: x[1], reverse=True)[:3]:\n",
    "    print(f\"  {sim:.4f}: '{p1}' vs '{p2}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bl_pos_with_sims = list(zip(positive_pairs, bl_pos))\n",
    "bl_neg_with_sims = list(zip(negative_pairs, bl_neg))\n",
    "\n",
    "print(\"BASELINE - Hardest Positive Pairs (lowest similarity, risk of cache miss):\")\n",
    "for (p1, p2), sim in sorted(bl_pos_with_sims, key=lambda x: x[1])[:3]:\n",
    "    print(f\"  {sim:.4f}: '{p1}' vs '{p2}'\")\n",
    "\n",
    "print(\"\\nBASELINE - Riskiest Negative Pairs (highest similarity, risk of false hit):\")\n",
    "for (p1, p2), sim in sorted(bl_neg_with_sims, key=lambda x: x[1], reverse=True)[:3]:\n",
    "    print(f\"  {sim:.4f}: '{p1}' vs '{p2}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Let's summarize what we found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal thresholds\n",
    "lc_best_idx = np.argmax([r[2] for r in lc_results])\n",
    "bl_best_idx = np.argmax([r[2] for r in bl_results])\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"EXPERIMENT SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nRedis LangCache (cache-optimized):\")\n",
    "print(f\"  Best threshold: {thresholds[lc_best_idx]}\")\n",
    "print(f\"  Best F1 score: {lc_results[lc_best_idx][2]:.3f}\")\n",
    "print(f\"  Separation gap: {np.mean(lc_pos) - np.mean(lc_neg):.4f}\")\n",
    "print(f\"  Embedding time: {lc_time:.3f}s\")\n",
    "\n",
    "print(\"\\nBaseline MiniLM (general-purpose):\")\n",
    "print(f\"  Best threshold: {thresholds[bl_best_idx]}\")\n",
    "print(f\"  Best F1 score: {bl_results[bl_best_idx][2]:.3f}\")\n",
    "print(f\"  Separation gap: {np.mean(bl_pos) - np.mean(bl_neg):.4f}\")\n",
    "print(f\"  Embedding time: {bl_time:.3f}s\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "winner = \"LangCache\" if lc_results[lc_best_idx][2] > bl_results[bl_best_idx][2] else \"Baseline\"\n",
    "diff = abs(lc_results[lc_best_idx][2] - bl_results[bl_best_idx][2])\n",
    "print(f\"Winner: {winner} (by {diff:.3f} F1 points)\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Conclusions\n\nOn this real estate query dataset:\n\n1. **Separation gap** tells us how distinguishable \"should match\" vs \"should not match\" pairs are\n2. **F1 score** balances precision (avoiding false cache hits) and recall (not missing valid cache hits)\n3. **Threshold selection** matters - the right threshold depends on your cost of false positives vs false negatives\n\n### Where Both Models Struggled\n\nReal estate jargon tripped up both models. \"Move-in ready home\" vs \"turnkey property no repairs needed\" scored low on both, even though these mean the same thing in real estate. The models learned general paraphrase patterns but not domain-specific vocabulary.\n\n### For Production Use\n\n- If false cache hits are expensive (wrong answers): prioritize **precision**, use higher threshold\n- If cache misses are expensive (unnecessary LLM calls): prioritize **recall**, use lower threshold\n- Remember you still pay for embedding generation and vector search on every cache lookup\n\n### Open Source Stack\n\nThe embedding models are Apache 2.0 licensed (separate from Redis's managed LangCache service):\n- [redis/langcache-embed-v3-small](https://huggingface.co/redis/langcache-embed-v3-small)\n- Run locally with sentence-transformers\n- Store embeddings in any vector database (Redis Stack, pgvector, etc.)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}